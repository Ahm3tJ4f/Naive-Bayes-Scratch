{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "author1 = ['For the Azerbaijani partners we have no restrictions for the moment.',\n",
    " 'You can travel, to Europe for mobility. Try to gather as many colleagues as possible at your end engaged in the realisation of the MAGnUS project.',\n",
    " 'It looks promising in some parts of the world and society tries to come back to normal.']\n",
    "author2 = ['We are all looking forward to your visit and Natasha and I are putting together an excellent programme.',\n",
    "' I know that some of you are arriving, some in transit and some on your way.',\n",
    "'One extremely important point: Each year we have someone who heads to Newcastle-upon-Tyne on the train or the bus ignoring my instructions and guidance.']\n",
    "text1 = 'As you can see from the attached report and accompanying letter we have received a good grade to sum up'.lower().split()\n",
    "text2 = 'Just a week before most of you will be setting off for the UK and for Keele. We are all looking forward'.lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "train_cv = cv.fit_transform(author1+author2)\n",
    "vocab_size = len(train_cv.toarray()[0])\n",
    "author1_size = np.sum(train_cv.toarray()[0:3])\n",
    "author2_size = np.sum(train_cv.toarray()[3:6])\n",
    "author1_text1_p = author2_text1_p = author2_text2_p= author1_text2_p =  0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in text1:\n",
    "    if(word in cv.get_feature_names()):\n",
    "        author1_text1_freq = train_cv.toarray()[0:3].sum(axis=0)[cv.get_feature_names().index(word)]\n",
    "        author2_text1_freq = train_cv.toarray()[3:6].sum(axis=0)[cv.get_feature_names().index(word)]\n",
    "        author1_text1_p *= (author1_text1_freq+1)/(author1_size+vocab_size)\n",
    "        author2_text1_p *= (author2_text1_freq+1)/(author2_size+vocab_size)\n",
    "\n",
    "for word in text2:\n",
    "    if(word in cv.get_feature_names()):\n",
    "        author1_text2_freq = train_cv.toarray()[0:3].sum(axis=0)[cv.get_feature_names().index(word)]\n",
    "        author2_text2_freq = train_cv.toarray()[3:6].sum(axis=0)[cv.get_feature_names().index(word)]\n",
    "        author1_text2_p *= (author1_text2_freq+1)/(author1_size+vocab_size)\n",
    "        author2_text2_p *= (author2_text2_freq+1)/(author2_size+vocab_size)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( author1_text1_p >  author2_text1_p):\n",
    "    print(' text 1 is written by author 1')\n",
    "else:\n",
    "    print(' text 1 is written by author 2')     \n",
    "    \n",
    "    \n",
    "if(author1_text2_p > author2_text2_p):\n",
    "    print(' text 2 is written by author 1')\n",
    "else:\n",
    "    print(' text 2 is written by author 2')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
